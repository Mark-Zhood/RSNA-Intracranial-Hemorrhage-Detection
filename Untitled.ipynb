{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intracranial Hemorrhage Detection\n",
    "\n",
    "This blog post is about the challenge that is hosted on kaggle on [RSNA Intracranial Hemorrhage Detection](https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection). \n",
    "\n",
    "This post is divided into following parts\n",
    "\n",
    "1. Overview\n",
    "2. Basic EDA [Ipython Notebook](https://www.kaggle.com/suryaparsa/rsna-basic-eda-part-1)\n",
    "3. Data Visualization & Preprocessing\n",
    "4. Deep Learning Model\n",
    "\n",
    "### 1. Overview\n",
    "\n",
    "##### What is Intracranial Hemorrhage?\n",
    "\n",
    "An intracranial hemorrhage is a type of bleeding that occurs inside the skull. Symptoms include sudden tingling, weakness, numbness, paralysis, severe headache, difficulty with swallowing or vision, loss of balance or coordination, difficulty understanding, speaking , reading, or writing, and a change in level of consciousness or alertness, marked by stupor, lethargy, sleepiness, or coma. Any type of bleeding inside the skull or brain is a medical emergency. It is important to get the person to a hospital emergency room immediately to determine the cause of the bleeding and begin medical treatment. It rquires highly trained specialists review medical images of the patientâ€™s cranium to look for the presence, location and type of hemorrhage. The process is complicated and often time consuming. So as part of this we will be deep learning techniques to detect acute intracranial hemorrhage and its subtypes.\n",
    "\n",
    "Hemorrhage Types\n",
    "\n",
    "1. Epidural\n",
    "2. Intraparenchymal    \n",
    "3. Intraventricular\n",
    "4. Subarachnoid \n",
    "5. Subdural\n",
    "6. Any\n",
    "7. Demo\n",
    "\n",
    "##### What am i predicting?\n",
    "\n",
    "In this competition our goal is to predict intracranial hemorrhage and its subtypes. Given an image the we need to predict probablity of each subtype. This indicates its a multilabel classification problem.\n",
    "\n",
    "##### Evaluation Metric\n",
    "\n",
    "Competition evaluation metric is **weighted log loss** but weights for each subtype is not disclosed as part of the competition but in the discussion forms some of the teams found it out that the any label has a weight of 2 compared to other subtypes, you can check more details [here](https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection/discussion/109526#latest-630190). But as part of this tutorial i'm going to use normal accuracy as evaluation metric and loss as **binary cross entropy loss** and checkpointing the models based on the loss.\n",
    "\n",
    "\n",
    "### 2. Basic EDA \n",
    "\n",
    "Lets look at the [data](https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection/data) that is provided.\n",
    "\n",
    "We have a train.csv containing file names and label indicating whether hemorrhage is present or not and train images folder which is set of [Dicom](https://www.dicomstandard.org/) files (Medical images are stored in dicom formats) and test images folder containing test dicom files.\n",
    "\n",
    "```python\n",
    "# load the csv file\n",
    "train_df = pd.read_csv(input_folder + 'stage_1_train.csv')\n",
    "train_df.head()\n",
    "```\n",
    "<img src='assets/df.png'/>\n",
    "          \n",
    "It consists of two columns ID and Label. ID has a format FILE_ID_SUB_TYPE for example ID_63eb1e259_epidural so ID_63eb1e259 is file id and epidural is subtype and Label indicating whether subtype hemorrhage is present or not.\n",
    "\n",
    "Lets seperate file names and subtypes\n",
    "\n",
    "```python\n",
    "# extract subtype\n",
    "train_df['sub_type'] = train_df['ID'].apply(lambda x: x.split('_')[-1])\n",
    "# extract filename\n",
    "train_df['file_name'] = train_df['ID'].apply(lambda x: '_'.join(x.split('_')[:2]) + '.dcm')\n",
    "train_df.head()\n",
    "```\n",
    "<img src='assets/df2.png'/>\n",
    "\n",
    "\n",
    "```python\n",
    "train_df.shape\n",
    "````\n",
    "Output : (4045572, 4)\n",
    "\n",
    "```python\n",
    "print(\"Number of train images availabe:\", len(os.listdir(path_train_img)))\n",
    "```\n",
    "Output : Number of train images availabe: 674258\n",
    "\n",
    "The csv file has a shape of (4045572, 4). For every file(dicom file) present in the train folder has 6 entries in csv indicating possible 6 subtype hemorrhages.\n",
    "\n",
    "Lets check the files available for each subtype\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(16, 6))\n",
    "graph = sns.countplot(x=\"sub_type\", hue=\"Label\", data=(train_df))\n",
    "graph.set_xticklabels(graph.get_xticklabels(),rotation=90)\n",
    "plt.show()\n",
    "```\n",
    "<img src='assets/counts.png'/>\n",
    "\n",
    "\n",
    "Lets check the counts for each subtype\n",
    "\n",
    "##### Epidural\n",
    "\n",
    "```python\n",
    "train_df[train_df['sub_type'] == 'epidural']['Label'].value_counts()\n",
    "```\n",
    "Output: \n",
    "\n",
    "0    671501\n",
    "\n",
    "1      2761\n",
    "\n",
    "Name: Label, dtype: int64\n",
    "\n",
    "For epidural sub type we have 6,71,501 images labeled as 0 and 2,761 labelled as 1.\n",
    "\n",
    "##### Intraparenchymal\n",
    "\n",
    "```python\n",
    "train_df[train_df['sub_type'] == 'intraparenchymal']['Label'].value_counts()\n",
    "```\n",
    "Output: <br/>\n",
    "0    641698<br/>\n",
    "1     32564<br/>\n",
    "Name: Label, dtype: int64\n",
    "\n",
    "For intraparenchymal sub type we have 6,41,698 images labeled as 0 and 32,564 labelled as 1.\n",
    "\n",
    "\n",
    "##### Intraparenchymal\n",
    "\n",
    "```python\n",
    "train_df[train_df['sub_type'] == 'intraparenchymal']['Label'].value_counts()\n",
    "```\n",
    "Output: <br/>\n",
    "0    650496<br/>\n",
    "1     23766<br/>\n",
    "Name: Label, dtype: int64\n",
    "\n",
    "For intraparenchymal sub type we have 6,50,496 images labeled as 0 and 23,766 labelled as 1.\n",
    "\n",
    "##### Subarachnoid\n",
    "\n",
    "```python\n",
    "train_df[train_df['sub_type'] == 'subarachnoid']['Label'].value_counts()\n",
    "```\n",
    "Output: <br/>\n",
    "0    642140<br/>\n",
    "1     32122<br/>\n",
    "Name: Label, dtype: int64\n",
    "\n",
    "For subarachnoid sub type we have 6,42,140 images labeled as 0 and 32,122 labelled as 1.\n",
    "\n",
    "\n",
    "##### Subdural\n",
    "\n",
    "```python\n",
    "train_df[train_df['sub_type'] == 'subdural']['Label'].value_counts()\n",
    "```\n",
    "Output: <br/>\n",
    "0    631766<br/>\n",
    "1     42496<br/>\n",
    "Name: Label, dtype: int64\n",
    "\n",
    "For Subdural sub type we have 6,31,766 images labeled as 0 and 42,496 labelled as 1.\n",
    "\n",
    "\n",
    "##### Any\n",
    "\n",
    "```python\n",
    "train_df[train_df['sub_type'] == 'any']['Label'].value_counts()\n",
    "```\n",
    "Output: <br/>\n",
    "0    577159<br/>\n",
    "1     97103<br/>\n",
    "Name: Label, dtype: int64\n",
    "\n",
    "For any sub type we have 5,77,159 images labeled as 0 and 97,103 labelled as 1.\n",
    "\n",
    "### 3. Data Visualization & Preprocessing\n",
    "\n",
    "Lets look at the dicom files in the dataset\n",
    "\n",
    "```python\n",
    "dicom = pydicom.read_file(path_train_img + 'ID_ffff922b9.dcm')\n",
    "print(dicom)\n",
    "```\n",
    "<img src='assets/dicom.png'/>\n",
    "\n",
    "\n",
    "Dicom data format files contain pixel data of image and other meta data like patient name, instance id, window width etc...\n",
    "\n",
    "Original image\n",
    "\n",
    "```python\n",
    "plt.imshow(dicom.pixel_array, cmap=plt.cm.bone)\n",
    "plt.show()\n",
    "```\n",
    "<img src='assets/original.png'/>\n",
    "\n",
    "\n",
    "The orginal image seems to have difficult to understand, lets check meta deta features like Window Center, Window Width, Rescale Intercept, Rescale Slope \n",
    "\n",
    "<img src='assets/meta.png'/>\n",
    "\n",
    "\n",
    "We can use these features to construct the new image.\n",
    "\n",
    "```python\n",
    "def get_dicom_field_value(key, dicom):\n",
    "    \"\"\"\n",
    "    @param key: key is tuple\n",
    "    @param dicom: dicom file\n",
    "    \"\"\"\n",
    "    return dicom[key].value\n",
    "\n",
    "window_center = int(get_dicom_field_value(('0028', '1050'), dicom))\n",
    "window_width = int(get_dicom_field_value(('0028', '1051'), dicom))\n",
    "window_intercept = int(get_dicom_field_value(('0028', '1052'), dicom))\n",
    "window_slope = int(get_dicom_field_value(('0028', '1053'), dicom))\n",
    "window_center, window_width, window_intercept, window_slope\n",
    "\n",
    "def get_windowed_image(image, wc,ww, intercept, slope):\n",
    "    img = (image*slope +intercept)\n",
    "    img_min = wc - ww//2\n",
    "    img_max = wc + ww//2\n",
    "    img[img<img_min] = img_min\n",
    "    img[img>img_max] = img_max\n",
    "    return img \n",
    "    \n",
    "windowed_image = get_windowed_image(dicom.pixel_array, window_center, window_width, \\\n",
    "                                    window_intercept, window_slope)\n",
    "                                    \n",
    "plt.imshow(windowed_image, cmap=plt.cm.bone)\n",
    "plt.show()\n",
    "```\n",
    "<img src='assets/windowed.png'/>\n",
    "\n",
    "\n",
    "\n",
    "The windowed image using meta data is much better than the orginal image this is because the dicom pixel array which contain pixel data contain raw data in Hounsfield units (HU). \n",
    "\n",
    "Scaling the image:\n",
    "\n",
    "Rescale the image to range 0-255.\n",
    "\n",
    "```python\n",
    "def get_scaled_windowed_image(img):\n",
    "    \"\"\"\n",
    "    Get scaled image\n",
    "    1. Convert to float\n",
    "    2. Rescale to 0-255\n",
    "    3. Convert to unit8\n",
    "    \"\"\"\n",
    "    img_2d = img.astype(float)\n",
    "    img_2d_scaled = (np.maximum(img_2d,0) / img_2d.max()) * 255.0\n",
    "    img_2d_scaled = np.uint8(img_2d_scaled)\n",
    "    return img_2d_scaled\n",
    "    \n",
    "scaled_image = get_scaled_windowed_image(windowed_image)\n",
    "plt.imshow(scaled_image, cmap=plt.cm.bone, vmin=0, vmax=255)\n",
    "plt.show()\n",
    "```\n",
    "<img src='assets/scaled.png'/>\n",
    "\n",
    "\n",
    "Hounsfield Units (HU) are the best source for constructing CT images. [Here](https://en.wikipedia.org/wiki/Hounsfield_scale) is detailed table showing the substance and HU range. \n",
    "\n",
    "A detailed explanation of all the possible windowing techniques can be found in this great kernel [(Gradient Sigmoid Windowing)](https://www.kaggle.com/reppic/gradient-sigmoid-windowing) \n",
    "\n",
    "```python\n",
    "\n",
    "def correct_dcm(dcm):\n",
    "    # Refer Jeremy Howard's Kernel https://www.kaggle.com/jhoward/from-prototyping-to-submission-fastai\n",
    "    x = dcm.pixel_array + 1000\n",
    "    px_mode = 4096\n",
    "    x[x>=px_mode] = x[x>=px_mode] - px_mode\n",
    "    dcm.PixelData = x.tobytes()\n",
    "    dcm.RescaleIntercept = -1000\n",
    "\n",
    "def window_image(dcm, window_center, window_width):\n",
    "    \n",
    "    if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n",
    "        correct_dcm(dcm)\n",
    "    \n",
    "    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n",
    "    img_min = window_center - window_width // 2\n",
    "    img_max = window_center + window_width // 2\n",
    "    img = np.clip(img, img_min, img_max)\n",
    "\n",
    "    return img\n",
    "\n",
    "def bsb_window(dcm):\n",
    "    brain_img = window_image(dcm, 40, 80)\n",
    "    subdural_img = window_image(dcm, 80, 200)\n",
    "    soft_img = window_image(dcm, 40, 380)\n",
    "    \n",
    "    brain_img = (brain_img - 0) / 80\n",
    "    subdural_img = (subdural_img - (-20)) / 200\n",
    "    soft_img = (soft_img - (-150)) / 380\n",
    "    bsb_img = np.array([brain_img, subdural_img, soft_img]).transpose(1,2,0)\n",
    "\n",
    "    return bsb_img\n",
    "    \n",
    "display_dicom_image('ID_0005d340e.dcm')\n",
    "```\n",
    "<img src='assets/dicom_all.png'/>\n",
    "\n",
    "\n",
    "It looks like Brain + Subdural is a good start for our models it has three chaneels and cab be easily fed to any pretrained models. \n",
    "\n",
    "\n",
    "### 4. Deep Learning Model\n",
    "\n",
    "The whole code for the training of the model can be found [here](/notebooks/Effnet-B0 Windowed Image.ipynb) \n",
    "\n",
    "We will using normal windowed images for training the model with augmentations like flip left right and random cropping.\n",
    "\n",
    "Here are steps for training the model\n",
    "\n",
    "1. Prepare train and validation data generators we will be splitting the data by stratifying the labels here id the link to [multilabel stratification](https://github.com/trent-b/iterative-stratification). We will make two splits and onlt work on the first split and check the results. \n",
    "2. Load pretrained Efficient Net B0 model.\n",
    "3. For the first epoch use all the train images for training the model with the first head layers using as it as is by setting trainable as False but train all the later images and save the model.\n",
    "4. Load the saved model and for the further epochs we train whole model except the last layer thus our model will learn most compliated features. \n",
    "5. Make predictions.\n",
    "\n",
    "Sample code:\n",
    "\n",
    "```python\n",
    "# 1. ---------prepare data generators-------------#\n",
    "# https://github.com/trent-b/iterative-stratification\n",
    "# Mutlilabel stratification\n",
    "splits = MultilabelStratifiedShuffleSplit(n_splits = 2, test_size = TEST_SIZE, random_state = SEED)\n",
    "file_names = train_final_df.index\n",
    "labels = train_final_df.values\n",
    "# Lets take only the first split\n",
    "split = next(splits.split(file_names, labels))\n",
    "train_idx = split[0]\n",
    "valid_idx = split[1]\n",
    "submission_predictions = []\n",
    "len(train_idx), len(valid_idx)\n",
    "# train data generator\n",
    "data_generator_train = TrainDataGenerator(train_final_df.iloc[train_idx], \n",
    "                                                train_final_df.iloc[train_idx], \n",
    "                                                TRAIN_BATCH_SIZE, \n",
    "                                                (WIDTH, HEIGHT),\n",
    "                                                augment = True)\n",
    "\n",
    "# validation data generator\n",
    "data_generator_val = TrainDataGenerator(train_final_df.iloc[valid_idx], \n",
    "                                            train_final_df.iloc[valid_idx], \n",
    "                                            VALID_BATCH_SIZE, \n",
    "                                            (WIDTH, HEIGHT),\n",
    "                                            augment = False)\n",
    "# 2. ---------load efficient net B0 model-----------#\n",
    "base_model =  efn.EfficientNetB0(weights = 'imagenet', include_top = False, \\\n",
    "                                 pooling = 'avg', input_shape = (HEIGHT, WIDTH, 3))\n",
    "x = base_model.output\n",
    "x = Dropout(0.125)(x)\n",
    "output_layer = Dense(6, activation = 'sigmoid')(x)\n",
    "model = Model(inputs=base_model.input, outputs=output_layer)\n",
    "model.compile(optimizer = Adam(learning_rate = 0.0001), \n",
    "                  loss = 'binary_crossentropy',\n",
    "                  metrics = ['acc', tf.keras.metrics.AUC()])\n",
    "model.summary()\n",
    "\n",
    "# 3. ---------for 1 st epoch train on whole dataset ------------#\n",
    "for layer in model.layers[:-5]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model.compile(optimizer = Adam(learning_rate = 0.0001), \n",
    "                  loss = 'binary_crossentropy',\n",
    "                  metrics = ['acc'])\n",
    "    \n",
    "model.fit_generator(generator = data_generator_train,\n",
    "                        validation_data = data_generator_val,\n",
    "                        epochs = 1,\n",
    "                        callbacks = callbacks_list,\n",
    "                        verbose = 1)\n",
    "\n",
    "# 4. ---------for rest of epochs train on sample data----------#\n",
    "model.load_weights('model.h5')\n",
    "model.compile(optimizer = Adam(learning_rate = 0.0004), \n",
    "                  loss = 'binary_crossentropy',\n",
    "                  metrics = ['acc'])\n",
    "model.fit_generator(generator = data_generator_train,\n",
    "                        validation_data = data_generator_val,\n",
    "                        steps_per_epoch=len(data_generator_train)/6,\n",
    "                        epochs = 10,\n",
    "                        callbacks = callbacks_list,\n",
    "                        verbose = 1)\n",
    "# 5. --------Make Predictions ------- --------------------------#\n",
    "model.load_weights('model.h5')\n",
    "\n",
    "def get_scores(data_gen, file_name='scores.pkl'):\n",
    "    scores = model.evaluate_generator(data_gen, verbose=1)\n",
    "    joblib.dump(scores, file_name)\n",
    "    print(f\"Loss: {scores[0]} and Accuracy: {scores[1]*100}\")\n",
    "```\n",
    "\n",
    "Lets predict on train and validation generators.\n",
    "\n",
    "```python\n",
    "get_scores(data_gen=data_generator_train, file_name='train_scores.pkl')\n",
    "```\n",
    "<img src='assets/train.png'/>\n",
    "\n",
    "```python\n",
    "get_scores(data_gen=data_generator_val, file_name='val_scores.pkl')\n",
    "```\n",
    "<img src='assets/val.png'/>\n",
    "\n",
    "Lets load test data frame, test data csv is also in the same format as train.csv\n",
    "\n",
    "```python\n",
    "# extract subtype\n",
    "test_df['sub_type'] = test_df['ID'].apply(lambda x: x.split('_')[-1])\n",
    "# extract filename\n",
    "test_df['file_name'] = test_df['ID'].apply(lambda x: '_'.join(x.split('_')[:2]) + '.dcm')\n",
    "\n",
    "test_df = pd.pivot_table(test_df.drop(columns='ID'), index=\"file_name\", \\\n",
    "                                columns=\"sub_type\", values=\"Label\")\n",
    "test_df.head()\n",
    "\n",
    "test_df.shape\n",
    "```\n",
    "\n",
    "Output: (78545, 6)\n",
    "\n",
    "So we have 78,545 test images and we need to predict 6 labels for each image. \n",
    "\n",
    "```python\n",
    "preds = model.predict_generator(TestDataGenerator(test_df.index, None, VALID_BATCH_SIZE, \\\n",
    "                                                  (WIDTH, HEIGHT), path_test_img), \n",
    "                                verbose=1)\n",
    "print(preds.shape)\n",
    "```\n",
    "Output: (78545, 6)\n",
    "\n",
    "\n",
    "As per sample submission given by kaggle it is in a different format, the submission should be made with ID and Label column where ID is in the form of <b>dicomId_subType</b>(Ex:ID_0fbf6a978_subarachnoid) so we need format this to convert each prediction to 6 rows each indicating the id with sub type and its probability. The following code generates the required format for submission.\n",
    "\n",
    "```python\n",
    "def create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n",
    "    \"\"\"\n",
    "    Helper function to generate download link to files in kaggle kernel \n",
    "    \"\"\"\n",
    "    html = '<a href={filename}>{title}</a>'\n",
    "    html = html.format(title=title,filename=filename)\n",
    "    return HTML(html)\n",
    "\n",
    "def generate_submission_file(preds):\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    cols = list(train_final_df.columns)\n",
    "\n",
    "    # We have preditions for each of the image\n",
    "    # We need to make 6 rows for each of file according to the subtype\n",
    "    ids = []\n",
    "    values = []\n",
    "    for i, j in tqdm(zip(preds, test_df.index.to_list()), total=preds.shape[0]):\n",
    "    #     print(i, j)\n",
    "        # i=[any_prob, epidural_prob, intraparenchymal_prob, intraventricular_prob, subarachnoid_prob, subdural_prob]\n",
    "        # j = filename ==> ID_xyz.dcm\n",
    "        for k in range(i.shape[0]):\n",
    "            ids.append([j.replace('.dcm', '_' + cols[k])])\n",
    "            values.append(i[k])      \n",
    "\n",
    "    df = pd.DataFrame(data=ids)\n",
    "    df.head()\n",
    "\n",
    "    sample_df = pd.read_csv(input_folder + 'stage_1_sample_submission.csv')\n",
    "    sample_df.head()\n",
    "\n",
    "    df['Label'] = values\n",
    "    df.columns = sample_df.columns\n",
    "    df.head()\n",
    "\n",
    "    df.to_csv('submission.csv', index=False)\n",
    "\n",
    "    return create_download_link(filename='submission.csv')\n",
    "```\n",
    "\n",
    "```python\n",
    "df = pd.read_csv('submission.csv')\n",
    "df.head()\n",
    "```\n",
    "\n",
    "<img src='assets/sample_sub.png'/>\n",
    "\n",
    "All notebooks can be found [here](https://github.com/suryachintu/RSNA-Intracranial-Hemorrhage-Detection/tree/master/notebooks)\n",
    "\n",
    "### 7. Demo\n",
    "\n",
    "You can test the model by uploading the DICOM file [here]('http://34.93.89.75:5325/)\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "https://my.clevelandclinic.org/health/diseases/14480-intracranial-hemorrhage-cerebral-hemorrhage-and-hemorrhagic-stroke<br/>\n",
    "https://github.com/MGH-LMIC/windows_optimization<br/>\n",
    "https://arxiv.org/abs/1812.00572(Must read)\n",
    "https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection/discussion/111325#latest-650043\n",
    "https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection/discussion/109261#latest-651855\n",
    "\n",
    "### Kaggle Kernels\n",
    "\n",
    "https://www.kaggle.com/jhoward/some-dicom-gotchas-to-be-aware-of-fastai\n",
    "https://www.kaggle.com/reppic/gradient-sigmoid-windowing\n",
    "https://www.kaggle.com/jhoward/from-prototyping-to-submission-fastai\n",
    "https://www.kaggle.com/suryaparsa/rsna-basic-eda-part-1\n",
    "https://www.kaggle.com/suryaparsa/rsna-basic-eda-part-2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
